{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "%matplotlib notebook\n",
    "tf.disable_v2_behavior()\n",
    "print(np.array([['X11','X12','X13','X14'],['X21','X22','X23','X24']]).shape)\n",
    "posdata = np.stack(([['X11','X12','X13','X14'],['X21','X22','X23','X24']],[['NN11','NN12','NN13','NN14'],['NN21','NN22','NN23','NN24']],[['NNN11','NNN12','NNN13','NNN14'],['NNN21','NNN22','NNN23','NNN24']]),axis=2)\n",
    "   \n",
    "posdata=np.reshape(posdata,(2,4*3))\n",
    "print(posdata)\n",
    "posdata=np.reshape(posdata[0],(4,3))\n",
    "print(posdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name,maxrow):\n",
    "    print(\"read data from \\\"\",file_name,\"\\\"\")\n",
    "    \n",
    "    xdata=np.loadtxt(file_name,max_rows=maxrow)\n",
    "    print(xdata.shape)\n",
    "    print(np.argwhere(np.isnan(xdata[:,1])))\n",
    "    xdata=np.delete(xdata,np.argwhere(np.isnan(xdata[:,1])),axis=0)\n",
    "    maxrow=xdata.shape[0]\n",
    "    print(xdata.shape)\n",
    "    L=(xdata.shape[1]-3)//9\n",
    "    print(L)\n",
    "    ydata=xdata[:,1]\n",
    "    dxdata=xdata[:,xdata.shape[1]-1]\n",
    "    \n",
    "    posdata=xdata[:,2+4*L:2+5*L]\n",
    "    NNdata=np.stack((xdata[:,2+L:2+2*L],xdata[:,2+3*L:2+4*L],xdata[:,2+5*L:2+6*L],xdata[:,2+7*L:2+8*L]),axis=2)\n",
    "    NNNdata=np.stack((xdata[:,2:2+L],xdata[:,2+2*L:2+3*L],xdata[:,2+6*L:2+7*L],xdata[:,2+8*L:2+9*L]),axis=2)\n",
    "    NNdata=np.mean(NNdata,axis=2)\n",
    "    NNNdata=np.mean(NNNdata,axis=2)\n",
    "    \n",
    "    averagex=np.mean(xdata[:,2:xdata.shape[1]-1])\n",
    "    print(averagex)\n",
    "    gdata_append=0.25*(posdata+np.outer(dxdata,np.ones(L))-averagex)**4-averagex**2/2.0*(posdata+np.outer(dxdata,np.ones(L))-averagex)**2-0.25*(posdata-averagex)**4+averagex**2/2.0*(posdata-averagex)**2\n",
    "    print(np.outer(dxdata,np.ones(L)).shape)\n",
    "    gdata_append=np.mean(gdata_append,axis=1)\n",
    "    zdata_append=np.sum((posdata+np.outer(dxdata,np.ones(L)))**2-posdata**2,axis=1)\n",
    "    print(posdata.shape)\n",
    "    posdata = np.stack((posdata,NNdata,NNNdata),axis=2)\n",
    "   \n",
    "    posdata=np.reshape(posdata,(maxrow,L*3))\n",
    "    \n",
    "    return posdata, dxdata, ydata, zdata_append, gdata_append\n",
    "\n",
    "\n",
    "def load_data2(file_name,maxrow):\n",
    "    print(\"read data from \\\"\",file_name,\"\\\"\")\n",
    "    \n",
    "    xdata=np.loadtxt(file_name,max_rows=maxrow)\n",
    "    L=(xdata.shape[1]-3)//9\n",
    "    print(L)\n",
    "    ydata=xdata[:,1]\n",
    "    dxdata=xdata[:,xdata.shape[1]-1]\n",
    "    \n",
    "    posdata=xdata[:,2+8*L:2+9*L]\n",
    "    NNdata=np.stack((xdata[:,2+L:2+2*L],xdata[:,2+3*L:2+4*L],xdata[:,2+4*L:2+5*L],xdata[:,2+6*L:2+7*L]),axis=2)\n",
    "    NNNdata=np.stack((xdata[:,2:2+L],xdata[:,2+2*L:2+3*L],xdata[:,2+5*L:2+6*L],xdata[:,2+7*L:2+8*L]),axis=2)\n",
    "    NNdata=np.mean(NNdata,axis=2)\n",
    "    NNNdata=np.mean(NNNdata,axis=2)\n",
    "    \n",
    "    averagex=np.mean(xdata[:,2:xdata.shape[1]-1])\n",
    "    print(averagex)\n",
    "    gdata_append=0.25*(posdata+np.outer(dxdata,np.ones(L))-averagex)**4-averagex**2/2.0*(posdata+np.outer(dxdata,np.ones(L))-averagex)**2-0.25*(posdata-averagex)**4+averagex**2/2.0*(posdata-averagex)**2\n",
    "    print(np.outer(dxdata,np.ones(L)).shape)\n",
    "    gdata_append=np.mean(gdata_append,axis=1)\n",
    "    zdata_append=np.sum((posdata+np.outer(dxdata,np.ones(L)))**2-posdata**2,axis=1)\n",
    "    print(posdata.shape)\n",
    "    posdata = np.stack((posdata,NNdata,NNNdata),axis=2)\n",
    "   \n",
    "    posdata=np.reshape(posdata,(maxrow,L*3))\n",
    "    \n",
    "    return posdata, dxdata, ydata, zdata_append, gdata_append\n",
    "\n",
    "\n",
    "\n",
    "def scaled_data(xdata):\n",
    "    xdata_mean=np.mean(xdata,axis=0)\n",
    "    xdata_std=np.std(xdata,axis=0)\n",
    "    xdata_scaled= (xdata-xdata_mean)/xdata_std\n",
    "    return xdata_scaled, xdata_mean, xdata_std\n",
    "\n",
    "\n",
    "def get_next_batch(x,y,start,end):\n",
    "    x_batch=x[start:end,0:x.shape[1]-3]\n",
    "    dx_batch=x[start:end,x.shape[1]-3]\n",
    "    z_batch=x[start:end,x.shape[1]-2]\n",
    "    g_batch=x[start:end,x.shape[1]-1]\n",
    "    y_batch=y[start:end]\n",
    "    return x_batch, dx_batch, z_batch, g_batch, y_batch\n",
    "\n",
    "\n",
    "def weight_variable(name, shape):\n",
    "    initer=tf.truncated_normal_initializer(stddev=0.02)\n",
    "    return tf.get_variable('W_'+name, dtype=tf.float32,shape=shape,initializer=initer)\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    initial=tf.constant(0.0,shape=shape,dtype=tf.float32)\n",
    "    return tf.get_variable('b_'+name,dtype=tf.float32,initializer=initial)\n",
    "\n",
    "def create_new_conv_layer(input_data, deltax, num_filters, filter_shape, name):\n",
    "    # setup the filter input shape for tf.nn.conv_2d\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], 1, num_filters]\n",
    "\n",
    "    # initialise weights and bias for the filter\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
    "                                      name=name+'_W')\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "\n",
    "    # setup the convolutional layer operation\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 3, 1, 1],padding='VALID')\n",
    "\n",
    "    # add the bias\n",
    "    out_layer += bias\n",
    "    \n",
    "    # add the theta term\n",
    "    in_dim=dx.get_shape()[1]\n",
    "    theta = tf.Variable(tf.truncated_normal([in_dim,num_filters]), name=name+'_theta')\n",
    "    out_layer +=tf.tensordot(tf.tensordot(tf.squeeze(dx),tf.ones([12],tf.float32),axes=0),theta,axes=0)\n",
    "\n",
    "    # apply a sigmoid non-linear activation\n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "\n",
    "    \n",
    "    return out_layer, weights, bias, theta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fc_layer(x,num_units,name,activation):\n",
    "    in_dim=x.get_shape()[1]\n",
    "    W=weight_variable(name,shape=[in_dim,num_units])\n",
    "    b=bias_variable(name,[num_units])\n",
    "    layer=tf.matmul(x,W)\n",
    "    layer+=b\n",
    "    if activation=='softplus':\n",
    "        layer=tf.nn.relu(layer)\n",
    "    elif activation=='sigmoid':\n",
    "        layer=tf.nn.relu(layer)\n",
    "    return layer, W, b\n",
    "\n",
    "def fc_layer_final(x,g,z,num_units,name):\n",
    "    in_dim=x.get_shape()[1]\n",
    "    W1=weight_variable(name,shape=[in_dim,num_units])\n",
    "    W2g=weight_variable(name+'g',shape=[1,num_units])\n",
    "    W3z=weight_variable(name+'x',shape=[1,num_units])\n",
    "    b=bias_variable(name,[num_units])\n",
    "    \n",
    "    layer=tf.matmul(x,W1)+tf.matmul(g,W2g)+tf.matmul(z,W3z)\n",
    "\n",
    "    layer+=b\n",
    "    return layer, W1, W2g, W3z, b\n",
    "\n",
    "def save_data(file_name,name,x,matrix):\n",
    "    out_file=open(file_name,'a')\n",
    "    if matrix:\n",
    "        out_file.write(\"%30s: [  %4d  ,  %4d  ]\\n\"%(name,x.shape[0], x.shape[1]))\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                out_file.write(\"%10.6f    \" %(x[i,j]))\n",
    "            out_file.write(\"\\n\")\n",
    "    else:\n",
    "        out_file.write(\"%s: [  %4d  ]\\n\"%(name,x.shape[0]))\n",
    "        for i in range(x.shape[0]):\n",
    "            out_file.write(\"%10.6f    \"%(x[i]))\n",
    "        out_file.write(\"\\n\")\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "file_name=\"./traindata/build/globaltrain.dat\"\n",
    "#file_name=\"./Desktop/block_train0003.dat\"\n",
    "\n",
    "maxrow=100000\n",
    "\n",
    "# set hyper parameters\n",
    "epochs=400\n",
    "batch_size=200\n",
    "display_freq=100\n",
    "learning_rate=0.0005\n",
    "L2_regularization=0.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "xdata, dxdata, ydata, zdata_append, gdata_append=load_data(file_name,maxrow)\n",
    "#xdata, dxdata, ydata, zdata_append, gdata_append=load_data2(file_name,maxrow)\n",
    "\n",
    "# scaled data\n",
    "xdata_scaled,        xdata_mean,        xdata_std        = scaled_data(xdata)\n",
    "zdata_append_scaled, zdata_append_mean, zdata_append_std = scaled_data(zdata_append)\n",
    "gdata_append_scaled, gdata_append_mean, gdata_append_std = scaled_data(gdata_append)\n",
    "dxdata_scaled,       dxdata_mean,       dxdata_std       = scaled_data(dxdata)\n",
    "xdata_combined=np.concatenate((xdata_scaled,np.array([dxdata_scaled]).T,np.array([zdata_append_scaled]).T,np.array([gdata_append_scaled]).T),axis=1)\n",
    "\n",
    "# split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(xdata_combined, ydata, test_size=0.33, random_state=42)\n",
    "L = (x_train.shape[1]-3)//3\n",
    "print(\"Size of:\")\n",
    "print('x_train:\\t{}'.format(x_train.shape))\n",
    "print('y_train:\\t{}'.format(y_train.shape))\n",
    "print('x_test:\\t{}'.format(x_test.shape))\n",
    "print('y_test:\\t{}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create placeholders\n",
    "x = tf.placeholder(tf.float32,shape=[None, x_train.shape[1]-3],name='X')\n",
    "x_shaped = tf.reshape(x,[-1,L,3,1])\n",
    "dx = tf.placeholder(tf.float32,shape=[None, 1] ,name='dX')\n",
    "g = tf.placeholder(tf.float32,shape=[None, 1] ,name='g')\n",
    "z = tf.placeholder(tf.float32,shape=[None, 1] ,name='z')\n",
    "y = tf.placeholder(tf.float32,shape=[None,1] ,name='Y')\n",
    "\n",
    "#define the network\n",
    "convout, Wconv, bconv, thetaconv = create_new_conv_layer(x_shaped, dx, 10, [8,3], name=\"convlayer\")\n",
    "flattened = tf.reshape(convout, [-1, 10*((L-8)//3+1)])\n",
    "fc1, W1, b1 = fc_layer(flattened, 20 , 'FC1' , 'sigmoid')\n",
    "fcib, Wib, bib = fc_layer(fc1, 10 , 'FCib' , 'sigmoid')\n",
    "\n",
    "output, W2, W2g, W2z, b2 = fc_layer_final(fcib,g,z,1,'FC2')\n",
    "\n",
    "\n",
    "loss=tf.reduce_mean(tf.squared_difference(output, y))\n",
    "regularizer=tf.reduce_sum([tf.nn.l2_loss(W1), tf.nn.l2_loss(W2), tf.nn.l2_loss(W2g), tf.nn.l2_loss(W2z), tf.nn.l2_loss(Wconv), tf.nn.l2_loss(thetaconv)])\n",
    "total_loss=tf.reduce_sum(loss+L2_regularization*regularizer)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate,name=\"Adam_op\").minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initializing all variables\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "num_tr_iter=int(len(y_train)/batch_size)\n",
    "print('num_tr_iter = ', num_tr_iter)\n",
    "store_loss_train_name='global_loss_train.dat'\n",
    "sltn=open(store_loss_train_name,\"w+\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('-----------------------------------------------')\n",
    "    print('Training epoch : ', epoch)\n",
    "    print('-----------------------------------------------')\n",
    "    perm = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(perm)\n",
    "    y_train = y_train[perm]\n",
    "    x_train = x_train[perm]\n",
    "    for iteration in range(num_tr_iter):\n",
    "        \n",
    "        start=iteration*batch_size\n",
    "        end=(iteration+1)*batch_size\n",
    "        x_batch,dx_batch,z_batch,g_batch,y_batch=get_next_batch(x_train,y_train,start,end)\n",
    "        z_batch=np.expand_dims(z_batch,1)\n",
    "        dx_batch=np.expand_dims(dx_batch,1)\n",
    "        g_batch=np.expand_dims(g_batch,1)\n",
    "        y_batch=np.expand_dims(y_batch,1)\n",
    "\n",
    "        feed_dict_batch={x:x_batch,dx:dx_batch,g:g_batch,z:z_batch,y:y_batch}\n",
    "\n",
    "        sess.run(optimizer,feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration%display_freq==0:\n",
    "            loss_batch=sess.run(loss,feed_dict=feed_dict_batch)\n",
    "            print(\"iter {0:3d}:\\t Loss = {1:2f}\".format(iteration,loss_batch))\n",
    "            sltn.write(\"%6d    %15.12f\\n\" %(iteration+epoch*num_tr_iter, loss_batch))\n",
    "sltn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss function\n",
    "#matplotlib.figure.Figure()\n",
    "xloss,yloss=np.loadtxt(store_loss_train_name,unpack=True)\n",
    "plt.plot(xloss, yloss)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Step',fontsize=18)\n",
    "plt.ylabel('Loss',fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "# save and plot y_predict_train, y_train\n",
    "store_y_train_name='y_train.dat'\n",
    "sytn=open(store_y_train_name,\"w+\")\n",
    "np.random.seed(68721)\n",
    "random_list=np.random.randint(len(y_train),size=2000)\n",
    "x_batch=np.zeros([1000,x_train.shape[1]-3])\n",
    "g_batch=np.zeros([1000,1])\n",
    "dx_batch=np.zeros([1000,1])\n",
    "z_batch=np.zeros([1000,1])\n",
    "y_batch=np.zeros([1000,1])\n",
    "for i in range(1000):\n",
    "    x_batch[i,0:x_train.shape[1]-3]=x_train[random_list[i],0:x_train.shape[1]-3].T\n",
    "    g_batch[i,0]=x_train[random_list[i],x_train.shape[1]-1]\n",
    "    z_batch[i,0]=x_train[random_list[i],x_train.shape[1]-2]\n",
    "    dx_batch[i,0]=x_train[random_list[i],x_train.shape[1]-3]\n",
    "    y_batch[i,0]=y_train[random_list[i]]\n",
    "\n",
    "feed_dict_batch={x:x_batch,dx:dx_batch,g:g_batch,z:z_batch,y:y_batch}\n",
    "y_train_predict=sess.run(output,feed_dict=feed_dict_batch)\n",
    "print(y_train_predict.shape)\n",
    "for i in range(1000):\n",
    "    sytn.write(\"%15.12f    %15.12f\\n\" %(y_batch[i,0], y_train_predict[i,0]))\n",
    "sytn.close()\n",
    "\n",
    "#matplotlib.figure.Figure()\n",
    "y1, y2=np.loadtxt(store_y_train_name,unpack=True)\n",
    "plt.plot(y1, y2,'ro')\n",
    "plt.plot(y1,y1,'-b')\n",
    "plt.xlabel('$\\Delta$E Value',fontsize=18)\n",
    "plt.ylabel('$\\Delta$E Prediction',fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch,dx_batch,z_batch,g_batch,y_batch=get_next_batch(x_test,y_test,0,len(y_test))\n",
    "dx_batch=np.expand_dims(dx_batch,1)\n",
    "y_batch=np.expand_dims(y_batch,1)\n",
    "g_batch=np.expand_dims(g_batch,1)\n",
    "z_batch=np.expand_dims(z_batch,1)\n",
    "\n",
    "feed_dict_batch={x:x_batch,dx:dx_batch,g:g_batch,z:z_batch,y:y_batch}\n",
    "loss_test=sess.run(loss,feed_dict=feed_dict_batch)\n",
    "print(\"Test Loss = \",loss_test,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"variable list:\\n\")\n",
    "print(tf.trainable_variables())\n",
    "W_conv_ = [v for v in tf.trainable_variables() if v.name == \"convlayer_W:0\"][0]\n",
    "b_conv_ = [v for v in tf.trainable_variables() if v.name == \"convlayer_b:0\"][0]\n",
    "t_conv_ = [v for v in tf.trainable_variables() if v.name == \"convlayer_theta:0\"][0]\n",
    "\n",
    "W_FC1_ = [v for v in tf.trainable_variables() if v.name == \"W_FC1:0\"][0]\n",
    "b_FC1_ = [v for v in tf.trainable_variables() if v.name == \"b_FC1:0\"][0]\n",
    "W_FC2_ = [v for v in tf.trainable_variables() if v.name == \"W_FC2:0\"][0]\n",
    "W_FC2g_ = [v for v in tf.trainable_variables() if v.name == \"W_FC2g:0\"][0]\n",
    "W_FC2z_ = [v for v in tf.trainable_variables() if v.name == \"W_FC2x:0\"][0]\n",
    "b_FC2_ = [v for v in tf.trainable_variables() if v.name == \"b_FC2:0\"][0]\n",
    "\n",
    "W_conv=W_conv_.eval(session=sess)\n",
    "b_conv=b_conv_.eval(session=sess)\n",
    "t_conv=t_conv_.eval(session=sess)\n",
    "\n",
    "W_FC1=W_FC1_.eval(session=sess)\n",
    "b_FC1=b_FC1_.eval(session=sess)\n",
    "W_FC2=W_FC2_.eval(session=sess)\n",
    "b_FC2=b_FC2_.eval(session=sess)\n",
    "W_FC2g=W_FC2g_.eval(session=sess)\n",
    "W_FC2z=W_FC2z_.eval(session=sess)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(gdata_append_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_FC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name=\"L41global_hyper_parameter.dat\"\n",
    "if os.path.exists(output_file_name):\n",
    "    os.remove(output_file_name)\n",
    "f = open(output_file_name, \"a\")\n",
    "for i in range(10):\n",
    "    for j in range(8):\n",
    "        for k in range(3):\n",
    "            f.write(str(W_conv[j,k,0,i])+' ')\n",
    "f.write('\\n')\n",
    "for i in range(120):\n",
    "    for j in range(20):\n",
    "        f.write(str(W_FC1[i,j])+' ')\n",
    "    f.write('\\n')\n",
    "for j in range(20):\n",
    "    f.write(str(W_FC2[j,0])+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(W_FC2g[0,0])+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(W_FC2z[0,0])+' ')\n",
    "f.write('\\n')\n",
    "for j in range(10):\n",
    "    f.write(str(t_conv[0,j])+' ')\n",
    "f.write('\\n')\n",
    "for j in range(10):\n",
    "    f.write(str(b_conv[j])+' ')\n",
    "f.write('\\n')\n",
    "for j in range(20):\n",
    "    f.write(str(b_FC1[j])+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(b_FC2[0])+' ')\n",
    "f.write('\\n')\n",
    "for i in range(123):\n",
    "    f.write(str(xdata_mean[i])+' ')\n",
    "f.write('\\n')\n",
    "for i in range(123):\n",
    "    f.write(str(xdata_std[i])+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(dxdata_mean)+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(dxdata_std)+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(zdata_append_mean)+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(zdata_append_std)+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(gdata_append_mean)+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(gdata_append_std)+' ')\n",
    "f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
