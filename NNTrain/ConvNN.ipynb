{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thomas/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "(2, 4)\n",
      "[['X11' 'NN11' 'NNN11' 'X12' 'NN12' 'NNN12' 'X13' 'NN13' 'NNN13' 'X14'\n",
      "  'NN14' 'NNN14']\n",
      " ['X21' 'NN21' 'NNN21' 'X22' 'NN22' 'NNN22' 'X23' 'NN23' 'NNN23' 'X24'\n",
      "  'NN24' 'NNN24']]\n",
      "[['X11' 'NN11' 'NNN11']\n",
      " ['X12' 'NN12' 'NNN12']\n",
      " ['X13' 'NN13' 'NNN13']\n",
      " ['X14' 'NN14' 'NNN14']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "tf.disable_v2_behavior()\n",
    "print(np.array([['X11','X12','X13','X14'],['X21','X22','X23','X24']]).shape)\n",
    "posdata = np.stack(([['X11','X12','X13','X14'],['X21','X22','X23','X24']],[['NN11','NN12','NN13','NN14'],['NN21','NN22','NN23','NN24']],[['NNN11','NNN12','NNN13','NNN14'],['NNN21','NNN22','NNN23','NNN24']]),axis=2)\n",
    "   \n",
    "posdata=np.reshape(posdata,(2,4*3))\n",
    "print(posdata)\n",
    "posdata=np.reshape(posdata[0],(4,3))\n",
    "print(posdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name,maxrow):\n",
    "    print(\"read data from \\\"\",file_name,\"\\\"\")\n",
    "    \n",
    "    xdata=np.loadtxt(file_name,max_rows=maxrow)\n",
    "    L=(xdata.shape[1]-3)//9\n",
    "    print(L)\n",
    "    ydata=xdata[:,1]\n",
    "    dxdata=xdata[:,xdata.shape[1]-1]\n",
    "    \n",
    "    posdata=xdata[:,2+4*L:2+5*L]\n",
    "    NNdata=np.stack((xdata[:,2+L:2+2*L],xdata[:,2+3*L:2+4*L],xdata[:,2+5*L:2+6*L],xdata[:,2+7*L:2+8*L]),axis=2)\n",
    "    NNNdata=np.stack((xdata[:,2:2+L],xdata[:,2+2*L:2+3*L],xdata[:,2+6*L:2+7*L],xdata[:,2+8*L:2+9*L]),axis=2)\n",
    "    NNdata=np.mean(NNdata,axis=2)\n",
    "    NNNdata=np.mean(NNNdata,axis=2)\n",
    "    \n",
    "    averagex=np.mean(xdata[:,2:xdata.shape[1]-1])\n",
    "    print(averagex)\n",
    "    gdata_append=0.25*(posdata+np.outer(dxdata,np.ones(L))-averagex)**4-averagex**2/2.0*(posdata+np.outer(dxdata,np.ones(L))-averagex)**2-0.25*(posdata-averagex)**4+averagex**2/2.0*(posdata-averagex)**2\n",
    "    print(np.outer(dxdata,np.ones(L)).shape)\n",
    "    gdata_append=np.mean(gdata_append,axis=1)\n",
    "    zdata_append=np.sum((posdata+np.outer(dxdata,np.ones(L)))**2-posdata**2,axis=1)\n",
    "    print(posdata.shape)\n",
    "    posdata = np.stack((posdata,NNdata,NNNdata),axis=2)\n",
    "   \n",
    "    posdata=np.reshape(posdata,(maxrow,L*3))\n",
    "    \n",
    "    return posdata, dxdata, ydata, zdata_append, gdata_append\n",
    "\n",
    "\n",
    "def scaled_data(xdata):\n",
    "    xdata_mean=np.mean(xdata,axis=0)\n",
    "    xdata_std=np.std(xdata,axis=0)\n",
    "    xdata_scaled= (xdata-xdata_mean)/xdata_std\n",
    "    return xdata_scaled, xdata_mean, xdata_std\n",
    "\n",
    "\n",
    "def get_next_batch(x,y,start,end):\n",
    "    x_batch=x[start:end,0:x.shape[1]-3]\n",
    "    dx_batch=x[start:end,x.shape[1]-3]\n",
    "    z_batch=x[start:end,x.shape[1]-2]\n",
    "    g_batch=x[start:end,x.shape[1]-1]\n",
    "    y_batch=y[start:end]\n",
    "    return x_batch, dx_batch, z_batch, g_batch, y_batch\n",
    "\n",
    "\n",
    "def weight_variable(name, shape):\n",
    "    initer=tf.truncated_normal_initializer(stddev=0.02)\n",
    "    return tf.get_variable('W_'+name, dtype=tf.float32,shape=shape,initializer=initer)\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    initial=tf.constant(0.0,shape=shape,dtype=tf.float32)\n",
    "    return tf.get_variable('b_'+name,dtype=tf.float32,initializer=initial)\n",
    "\n",
    "def create_new_conv_layer(input_data, deltax, num_filters, filter_shape, name):\n",
    "    # setup the filter input shape for tf.nn.conv_2d\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], 1, num_filters]\n",
    "\n",
    "    # initialise weights and bias for the filter\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
    "                                      name=name+'_W')\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "\n",
    "    # setup the convolutional layer operation\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 3, 1, 1],padding='VALID')\n",
    "\n",
    "    # add the bias\n",
    "    out_layer += bias\n",
    "    \n",
    "    # add the theta term\n",
    "    in_dim=dx.get_shape()[1]\n",
    "    theta = tf.Variable(tf.truncated_normal([in_dim,num_filters]), name=name+'_theta')\n",
    "    print_out_layer=tf.Print(out_layer,[tf.shape(out_layer)],\"This is the shape of that out_layer, should be 100, 12,1,5: \",summarize=4)\n",
    "    print_out_layer +=tf.tensordot(tf.tensordot(tf.squeeze(dx),tf.ones([12],tf.float32),axes=0),theta,axes=0)\n",
    "\n",
    "    # apply a sigmoid non-linear activation\n",
    "    print_out_layer = tf.nn.sigmoid(print_out_layer)\n",
    "\n",
    "    \n",
    "    return print_out_layer, weights, bias, theta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fc_layer(x,num_units,name,activation):\n",
    "    in_dim=x.get_shape()[1]\n",
    "    W=weight_variable(name,shape=[in_dim,num_units])\n",
    "    b=bias_variable(name,[num_units])\n",
    "    layer=tf.matmul(x,W)\n",
    "    layer+=b\n",
    "    if activation=='softplus':\n",
    "        layer=tf.nn.softplus(layer)\n",
    "    elif activation=='sigmoid':\n",
    "        layer=tf.nn.sigmoid(layer)\n",
    "    return layer, W, b\n",
    "\n",
    "def fc_layer_final(x,g,z,num_units,name):\n",
    "    in_dim=x.get_shape()[1]\n",
    "    W1=weight_variable(name,shape=[in_dim,num_units])\n",
    "    W2g=weight_variable(name+'g',shape=[1,num_units])\n",
    "    W3z=weight_variable(name+'x',shape=[1,num_units])\n",
    "    b=bias_variable(name,[num_units])\n",
    "    print_x=tf.Print(x,[tf.shape(x)],\"This is the shape of the x value: \")\n",
    "    print_g=tf.Print(g,[g],\"This is the g value: \")\n",
    "    print_z=tf.Print(z,[z],\"This is the z value: \")\n",
    "    print_W3z=tf.Print(W3z,[W3z],\"This is the W3z value: \")\n",
    "    \n",
    "    layer=tf.matmul(print_x,W1)+tf.matmul(print_g,W2g)+tf.matmul(print_z,print_W3z)\n",
    "\n",
    "    print_layer=tf.Print(layer,[layer],\"This is the pre b layer value: \")\n",
    "    print_layer+=b\n",
    "    return print_layer, W1, W2g, print_W3z, b\n",
    "\n",
    "def save_data(file_name,name,x,matrix):\n",
    "    out_file=open(file_name,'a')\n",
    "    if matrix:\n",
    "        out_file.write(\"%30s: [  %4d  ,  %4d  ]\\n\"%(name,x.shape[0], x.shape[1]))\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                out_file.write(\"%10.6f    \" %(x[i,j]))\n",
    "            out_file.write(\"\\n\")\n",
    "    else:\n",
    "        out_file.write(\"%s: [  %4d  ]\\n\"%(name,x.shape[0]))\n",
    "        for i in range(x.shape[0]):\n",
    "            out_file.write(\"%10.6f    \"%(x[i]))\n",
    "        out_file.write(\"\\n\")\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "file_name=\"./traindata/build/globaltrain.dat\"\n",
    "#file_name=\"./Desktop/block_train0003.dat\"\n",
    "\n",
    "maxrow=80001\n",
    "\n",
    "# set hyper parameters\n",
    "epochs=50\n",
    "batch_size=100\n",
    "display_freq=1\n",
    "learning_rate=0.001\n",
    "L2_regularization=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data from \" ./traindata/build/globaltrain.dat \"\n",
      "41\n",
      "-3.999213336841781\n",
      "(80000, 41)\n",
      "(80000, 41)\n",
      "Size of:\n",
      "x_train:\t(53600, 126)\n",
      "y_train:\t(53600,)\n",
      "x_test:\t(26400, 126)\n",
      "y_test:\t(26400,)\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "xdata, dxdata, ydata, zdata_append, gdata_append=load_data(file_name,maxrow)\n",
    "\n",
    "# scaled data\n",
    "xdata_scaled,        xdata_mean,        xdata_std        = scaled_data(xdata)\n",
    "zdata_append_scaled, zdata_append_mean, zdata_append_std = scaled_data(zdata_append)\n",
    "gdata_append_scaled, gdata_append_mean, gdata_append_std = scaled_data(gdata_append)\n",
    "dxdata_scaled,       dxdata_mean,       dxdata_std       = scaled_data(dxdata)\n",
    "xdata_combined=np.concatenate((xdata_scaled,np.array([dxdata_scaled]).T,np.array([zdata_append_scaled]).T,np.array([gdata_append_scaled]).T),axis=1)\n",
    "\n",
    "# split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(xdata_combined, ydata, test_size=0.33, random_state=42)\n",
    "L = (x_train.shape[1]-3)//3\n",
    "print(\"Size of:\")\n",
    "print('x_train:\\t{}'.format(x_train.shape))\n",
    "print('y_train:\\t{}'.format(y_train.shape))\n",
    "print('x_test:\\t{}'.format(x_test.shape))\n",
    "print('y_test:\\t{}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ydata = np.delete(ydata,np.argwhere(np.isnan(ydata)))\n",
    "print(np.argwhere(np.isnan(ydata)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-59841d08ce0b>:72: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
      "Instructions for updating:\n",
      "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
      "\n",
      "WARNING:tensorflow:From /home/thomas/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py:4022: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n"
     ]
    }
   ],
   "source": [
    "#create placeholders\n",
    "x = tf.placeholder(tf.float32,shape=[None, x_train.shape[1]-3],name='X')\n",
    "x_shaped = tf.reshape(x,[-1,L,3,1])\n",
    "dx = tf.placeholder(tf.float32,shape=[None, 1] ,name='dX')\n",
    "g = tf.placeholder(tf.float32,shape=[None, 1] ,name='g')\n",
    "z = tf.placeholder(tf.float32,shape=[None, 1] ,name='z')\n",
    "y = tf.placeholder(tf.float32,shape=[None,1] ,name='Y')\n",
    "\n",
    "#define the network\n",
    "convout, Wconv, bconv, thetaconv = create_new_conv_layer(x_shaped, dx, 1, [8,3], name=\"convlayer\")\n",
    "flattened = tf.reshape(convout, [-1, 1*((L-8)//3+1)])\n",
    "fc1, W1, b1 = fc_layer(flattened, 10 , 'FC1' , 'sigmoid')\n",
    "output, W2, W2g, W2z, b2 = fc_layer_final(fc1,g,z,1,'FC2')\n",
    "\n",
    "print_output = tf.Print(output,[output],\"output is: \")\n",
    "print_y = tf.Print(y,[y],\"actual value is: \")\n",
    "\n",
    "loss=tf.reduce_mean(tf.squared_difference(print_output, print_y))\n",
    "regularizer=tf.reduce_sum([tf.nn.l2_loss(W1), tf.nn.l2_loss(W2), tf.nn.l2_loss(W2g), tf.nn.l2_loss(W2z), tf.nn.l2_loss(Wconv), tf.nn.l2_loss(thetaconv)])\n",
    "total_loss=tf.reduce_sum(loss+L2_regularization*regularizer)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate,name=\"Adam_op\").minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tr_iter =  53\n",
      "-----------------------------------------------\n",
      "Training epoch :  0\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = nan\n",
      "iter   1:\t Loss = nan\n",
      "iter   2:\t Loss = nan\n",
      "iter   3:\t Loss = nan\n",
      "iter   4:\t Loss = nan\n",
      "iter   5:\t Loss = nan\n",
      "iter   6:\t Loss = nan\n",
      "iter   7:\t Loss = nan\n",
      "iter   8:\t Loss = nan\n",
      "iter   9:\t Loss = nan\n",
      "iter  10:\t Loss = nan\n",
      "iter  11:\t Loss = nan\n",
      "iter  12:\t Loss = nan\n",
      "iter  13:\t Loss = nan\n",
      "iter  14:\t Loss = nan\n",
      "iter  15:\t Loss = nan\n",
      "iter  16:\t Loss = nan\n",
      "iter  17:\t Loss = nan\n",
      "iter  18:\t Loss = nan\n",
      "iter  19:\t Loss = nan\n",
      "iter  20:\t Loss = nan\n",
      "iter  21:\t Loss = nan\n",
      "iter  22:\t Loss = nan\n",
      "iter  23:\t Loss = nan\n",
      "iter  24:\t Loss = nan\n",
      "iter  25:\t Loss = nan\n",
      "iter  26:\t Loss = nan\n",
      "iter  27:\t Loss = nan\n",
      "iter  28:\t Loss = nan\n",
      "iter  29:\t Loss = nan\n",
      "iter  30:\t Loss = nan\n",
      "iter  31:\t Loss = nan\n",
      "iter  32:\t Loss = nan\n",
      "iter  33:\t Loss = nan\n",
      "iter  34:\t Loss = nan\n",
      "iter  35:\t Loss = nan\n",
      "iter  36:\t Loss = nan\n",
      "iter  37:\t Loss = nan\n",
      "iter  38:\t Loss = nan\n",
      "iter  39:\t Loss = nan\n",
      "iter  40:\t Loss = nan\n",
      "iter  41:\t Loss = nan\n",
      "iter  42:\t Loss = nan\n",
      "iter  43:\t Loss = nan\n",
      "iter  44:\t Loss = nan\n",
      "iter  45:\t Loss = nan\n",
      "iter  46:\t Loss = nan\n",
      "iter  47:\t Loss = nan\n",
      "iter  48:\t Loss = nan\n",
      "iter  49:\t Loss = nan\n",
      "iter  50:\t Loss = nan\n",
      "iter  51:\t Loss = nan\n",
      "iter  52:\t Loss = nan\n",
      "-----------------------------------------------\n",
      "Training epoch :  1\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = nan\n",
      "iter   1:\t Loss = nan\n",
      "iter   2:\t Loss = nan\n",
      "iter   3:\t Loss = nan\n",
      "iter   4:\t Loss = nan\n",
      "iter   5:\t Loss = nan\n",
      "iter   6:\t Loss = nan\n",
      "iter   7:\t Loss = nan\n",
      "iter   8:\t Loss = nan\n",
      "iter   9:\t Loss = nan\n",
      "iter  10:\t Loss = nan\n",
      "iter  11:\t Loss = nan\n",
      "iter  12:\t Loss = nan\n",
      "iter  13:\t Loss = nan\n",
      "iter  14:\t Loss = nan\n",
      "iter  15:\t Loss = nan\n",
      "iter  16:\t Loss = nan\n",
      "iter  17:\t Loss = nan\n",
      "iter  18:\t Loss = nan\n",
      "iter  19:\t Loss = nan\n",
      "iter  20:\t Loss = nan\n",
      "iter  21:\t Loss = nan\n",
      "iter  22:\t Loss = nan\n",
      "iter  23:\t Loss = nan\n",
      "iter  24:\t Loss = nan\n",
      "iter  25:\t Loss = nan\n",
      "iter  26:\t Loss = nan\n",
      "iter  27:\t Loss = nan\n",
      "iter  28:\t Loss = nan\n",
      "iter  29:\t Loss = nan\n",
      "iter  30:\t Loss = nan\n",
      "iter  31:\t Loss = nan\n",
      "iter  32:\t Loss = nan\n",
      "iter  33:\t Loss = nan\n",
      "iter  34:\t Loss = nan\n",
      "iter  35:\t Loss = nan\n",
      "iter  36:\t Loss = nan\n",
      "iter  37:\t Loss = nan\n",
      "iter  38:\t Loss = nan\n",
      "iter  39:\t Loss = nan\n",
      "iter  40:\t Loss = nan\n",
      "iter  41:\t Loss = nan\n",
      "iter  42:\t Loss = nan\n",
      "iter  43:\t Loss = nan\n",
      "iter  44:\t Loss = nan\n",
      "iter  45:\t Loss = nan\n",
      "iter  46:\t Loss = nan\n",
      "iter  47:\t Loss = nan\n",
      "iter  48:\t Loss = nan\n",
      "iter  49:\t Loss = nan\n",
      "iter  50:\t Loss = nan\n",
      "iter  51:\t Loss = nan\n",
      "iter  52:\t Loss = nan\n",
      "-----------------------------------------------\n",
      "Training epoch :  2\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = nan\n",
      "iter   1:\t Loss = nan\n",
      "iter   2:\t Loss = nan\n",
      "iter   3:\t Loss = nan\n",
      "iter   4:\t Loss = nan\n",
      "iter   5:\t Loss = nan\n",
      "iter   6:\t Loss = nan\n",
      "iter   7:\t Loss = nan\n",
      "iter   8:\t Loss = nan\n",
      "iter   9:\t Loss = nan\n",
      "iter  10:\t Loss = nan\n",
      "iter  11:\t Loss = nan\n",
      "iter  12:\t Loss = nan\n",
      "iter  13:\t Loss = nan\n",
      "iter  14:\t Loss = nan\n",
      "iter  15:\t Loss = nan\n",
      "iter  16:\t Loss = nan\n",
      "iter  17:\t Loss = nan\n",
      "iter  18:\t Loss = nan\n",
      "iter  19:\t Loss = nan\n",
      "iter  20:\t Loss = nan\n",
      "iter  21:\t Loss = nan\n",
      "iter  22:\t Loss = nan\n",
      "iter  23:\t Loss = nan\n",
      "iter  24:\t Loss = nan\n",
      "iter  25:\t Loss = nan\n",
      "iter  26:\t Loss = nan\n",
      "iter  27:\t Loss = nan\n",
      "iter  28:\t Loss = nan\n",
      "iter  29:\t Loss = nan\n",
      "iter  30:\t Loss = nan\n",
      "iter  31:\t Loss = nan\n",
      "iter  32:\t Loss = nan\n",
      "iter  33:\t Loss = nan\n",
      "iter  34:\t Loss = nan\n",
      "iter  35:\t Loss = nan\n",
      "iter  36:\t Loss = nan\n",
      "iter  37:\t Loss = nan\n",
      "iter  38:\t Loss = nan\n",
      "iter  39:\t Loss = nan\n",
      "iter  40:\t Loss = nan\n",
      "iter  41:\t Loss = nan\n",
      "iter  42:\t Loss = nan\n",
      "iter  43:\t Loss = nan\n",
      "iter  44:\t Loss = nan\n",
      "iter  45:\t Loss = nan\n",
      "iter  46:\t Loss = nan\n",
      "iter  47:\t Loss = nan\n",
      "iter  48:\t Loss = nan\n",
      "iter  49:\t Loss = nan\n",
      "iter  50:\t Loss = nan\n",
      "iter  51:\t Loss = nan\n",
      "iter  52:\t Loss = nan\n",
      "-----------------------------------------------\n",
      "Training epoch :  3\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = nan\n",
      "iter   1:\t Loss = nan\n",
      "iter   2:\t Loss = nan\n",
      "iter   3:\t Loss = nan\n",
      "iter   4:\t Loss = nan\n",
      "iter   5:\t Loss = nan\n",
      "iter   6:\t Loss = nan\n",
      "iter   7:\t Loss = nan\n",
      "iter   8:\t Loss = nan\n",
      "iter   9:\t Loss = nan\n",
      "iter  10:\t Loss = nan\n",
      "iter  11:\t Loss = nan\n",
      "iter  12:\t Loss = nan\n",
      "iter  13:\t Loss = nan\n",
      "iter  14:\t Loss = nan\n",
      "iter  15:\t Loss = nan\n",
      "iter  16:\t Loss = nan\n",
      "iter  17:\t Loss = nan\n",
      "iter  18:\t Loss = nan\n",
      "iter  19:\t Loss = nan\n",
      "iter  20:\t Loss = nan\n",
      "iter  21:\t Loss = nan\n",
      "iter  22:\t Loss = nan\n",
      "iter  23:\t Loss = nan\n",
      "iter  24:\t Loss = nan\n",
      "iter  25:\t Loss = nan\n",
      "iter  26:\t Loss = nan\n",
      "iter  27:\t Loss = nan\n",
      "iter  28:\t Loss = nan\n",
      "iter  29:\t Loss = nan\n",
      "iter  30:\t Loss = nan\n",
      "iter  31:\t Loss = nan\n",
      "iter  32:\t Loss = nan\n",
      "iter  33:\t Loss = nan\n",
      "iter  34:\t Loss = nan\n",
      "iter  35:\t Loss = nan\n",
      "iter  36:\t Loss = nan\n",
      "iter  37:\t Loss = nan\n",
      "iter  38:\t Loss = nan\n",
      "iter  39:\t Loss = nan\n",
      "iter  40:\t Loss = nan\n",
      "iter  41:\t Loss = nan\n",
      "iter  42:\t Loss = nan\n",
      "iter  43:\t Loss = nan\n",
      "iter  44:\t Loss = nan\n",
      "iter  45:\t Loss = nan\n",
      "iter  46:\t Loss = nan\n",
      "iter  47:\t Loss = nan\n",
      "iter  48:\t Loss = nan\n",
      "iter  49:\t Loss = nan\n",
      "iter  50:\t Loss = nan\n",
      "iter  51:\t Loss = nan\n",
      "iter  52:\t Loss = nan\n",
      "-----------------------------------------------\n",
      "Training epoch :  4\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = nan\n",
      "iter   1:\t Loss = nan\n",
      "iter   2:\t Loss = nan\n",
      "iter   3:\t Loss = nan\n",
      "iter   4:\t Loss = nan\n",
      "iter   5:\t Loss = nan\n",
      "iter   6:\t Loss = nan\n",
      "iter   7:\t Loss = nan\n",
      "iter   8:\t Loss = nan\n",
      "iter   9:\t Loss = nan\n",
      "iter  10:\t Loss = nan\n",
      "iter  11:\t Loss = nan\n",
      "iter  12:\t Loss = nan\n",
      "iter  13:\t Loss = nan\n",
      "iter  14:\t Loss = nan\n",
      "iter  15:\t Loss = nan\n",
      "iter  16:\t Loss = nan\n",
      "iter  17:\t Loss = nan\n",
      "iter  18:\t Loss = nan\n",
      "iter  19:\t Loss = nan\n",
      "iter  20:\t Loss = nan\n",
      "iter  21:\t Loss = nan\n",
      "iter  22:\t Loss = nan\n",
      "iter  23:\t Loss = nan\n",
      "iter  24:\t Loss = nan\n",
      "iter  25:\t Loss = nan\n",
      "iter  26:\t Loss = nan\n",
      "iter  27:\t Loss = nan\n",
      "iter  28:\t Loss = nan\n",
      "iter  29:\t Loss = nan\n",
      "iter  30:\t Loss = nan\n",
      "iter  31:\t Loss = nan\n",
      "iter  32:\t Loss = nan\n",
      "iter  33:\t Loss = nan\n",
      "iter  34:\t Loss = nan\n",
      "iter  35:\t Loss = nan\n",
      "iter  36:\t Loss = nan\n",
      "iter  37:\t Loss = nan\n",
      "iter  38:\t Loss = nan\n",
      "iter  39:\t Loss = nan\n",
      "iter  40:\t Loss = nan\n",
      "iter  41:\t Loss = nan\n",
      "iter  42:\t Loss = nan\n",
      "iter  43:\t Loss = nan\n",
      "iter  44:\t Loss = nan\n",
      "iter  45:\t Loss = nan\n",
      "iter  46:\t Loss = nan\n",
      "iter  47:\t Loss = nan\n",
      "iter  48:\t Loss = nan\n",
      "iter  49:\t Loss = nan\n",
      "iter  50:\t Loss = nan\n",
      "iter  51:\t Loss = nan\n",
      "iter  52:\t Loss = nan\n",
      "-----------------------------------------------\n",
      "Training epoch :  5\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = nan\n",
      "iter   1:\t Loss = nan\n",
      "iter   2:\t Loss = nan\n",
      "iter   3:\t Loss = nan\n",
      "iter   4:\t Loss = nan\n",
      "iter   5:\t Loss = nan\n",
      "iter   6:\t Loss = nan\n",
      "iter   7:\t Loss = nan\n",
      "iter   8:\t Loss = nan\n",
      "iter   9:\t Loss = nan\n",
      "iter  10:\t Loss = nan\n",
      "iter  11:\t Loss = nan\n",
      "iter  12:\t Loss = nan\n",
      "iter  13:\t Loss = nan\n",
      "iter  14:\t Loss = nan\n",
      "iter  15:\t Loss = nan\n",
      "iter  16:\t Loss = nan\n",
      "iter  17:\t Loss = nan\n",
      "iter  18:\t Loss = nan\n",
      "iter  19:\t Loss = nan\n",
      "iter  20:\t Loss = nan\n",
      "iter  21:\t Loss = nan\n",
      "iter  22:\t Loss = nan\n",
      "iter  23:\t Loss = nan\n",
      "iter  24:\t Loss = nan\n",
      "iter  25:\t Loss = nan\n",
      "iter  26:\t Loss = nan\n",
      "iter  27:\t Loss = nan\n",
      "iter  28:\t Loss = nan\n",
      "iter  29:\t Loss = nan\n",
      "iter  30:\t Loss = nan\n",
      "iter  31:\t Loss = nan\n",
      "iter  32:\t Loss = nan\n",
      "iter  33:\t Loss = nan\n",
      "iter  34:\t Loss = nan\n",
      "iter  35:\t Loss = nan\n",
      "iter  36:\t Loss = nan\n",
      "iter  37:\t Loss = nan\n",
      "iter  38:\t Loss = nan\n",
      "iter  39:\t Loss = nan\n",
      "iter  40:\t Loss = nan\n",
      "iter  41:\t Loss = nan\n",
      "iter  42:\t Loss = nan\n",
      "iter  43:\t Loss = nan\n",
      "iter  44:\t Loss = nan\n",
      "iter  45:\t Loss = nan\n",
      "iter  46:\t Loss = nan\n",
      "iter  47:\t Loss = nan\n",
      "iter  48:\t Loss = nan\n",
      "iter  49:\t Loss = nan\n",
      "iter  50:\t Loss = nan\n",
      "iter  51:\t Loss = nan\n",
      "iter  52:\t Loss = nan\n",
      "-----------------------------------------------\n",
      "Training epoch :  6\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = nan\n",
      "iter   1:\t Loss = nan\n",
      "iter   2:\t Loss = nan\n",
      "iter   3:\t Loss = nan\n",
      "iter   4:\t Loss = nan\n",
      "iter   5:\t Loss = nan\n",
      "iter   6:\t Loss = nan\n",
      "iter   7:\t Loss = nan\n",
      "iter   8:\t Loss = nan\n",
      "iter   9:\t Loss = nan\n",
      "iter  10:\t Loss = nan\n",
      "iter  11:\t Loss = nan\n",
      "iter  12:\t Loss = nan\n",
      "iter  13:\t Loss = nan\n",
      "iter  14:\t Loss = nan\n",
      "iter  15:\t Loss = nan\n",
      "iter  16:\t Loss = nan\n",
      "iter  17:\t Loss = nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  18:\t Loss = nan\n",
      "iter  19:\t Loss = nan\n",
      "iter  20:\t Loss = nan\n",
      "iter  21:\t Loss = nan\n",
      "iter  22:\t Loss = nan\n",
      "iter  23:\t Loss = nan\n",
      "iter  24:\t Loss = nan\n",
      "iter  25:\t Loss = nan\n",
      "iter  26:\t Loss = nan\n",
      "iter  27:\t Loss = nan\n",
      "iter  28:\t Loss = nan\n",
      "iter  29:\t Loss = nan\n",
      "iter  30:\t Loss = nan\n",
      "iter  31:\t Loss = nan\n",
      "iter  32:\t Loss = nan\n",
      "iter  33:\t Loss = nan\n",
      "iter  34:\t Loss = nan\n",
      "iter  35:\t Loss = nan\n",
      "iter  36:\t Loss = nan\n",
      "iter  37:\t Loss = nan\n",
      "iter  38:\t Loss = nan\n",
      "iter  39:\t Loss = nan\n",
      "iter  40:\t Loss = nan\n",
      "iter  41:\t Loss = nan\n",
      "iter  42:\t Loss = nan\n",
      "iter  43:\t Loss = nan\n",
      "iter  44:\t Loss = nan\n",
      "iter  45:\t Loss = nan\n",
      "iter  46:\t Loss = nan\n",
      "iter  47:\t Loss = nan\n",
      "iter  48:\t Loss = nan\n",
      "iter  49:\t Loss = nan\n",
      "iter  50:\t Loss = nan\n",
      "iter  51:\t Loss = nan\n",
      "iter  52:\t Loss = nan\n",
      "-----------------------------------------------\n",
      "Training epoch :  7\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = nan\n",
      "iter   1:\t Loss = nan\n",
      "iter   2:\t Loss = nan\n",
      "iter   3:\t Loss = nan\n",
      "iter   4:\t Loss = nan\n",
      "iter   5:\t Loss = nan\n",
      "iter   6:\t Loss = nan\n",
      "iter   7:\t Loss = nan\n",
      "iter   8:\t Loss = nan\n",
      "iter   9:\t Loss = nan\n",
      "iter  10:\t Loss = nan\n",
      "iter  11:\t Loss = nan\n",
      "iter  12:\t Loss = nan\n",
      "iter  13:\t Loss = nan\n",
      "iter  14:\t Loss = nan\n",
      "iter  15:\t Loss = nan\n",
      "iter  16:\t Loss = nan\n",
      "iter  17:\t Loss = nan\n",
      "iter  18:\t Loss = nan\n",
      "iter  19:\t Loss = nan\n",
      "iter  20:\t Loss = nan\n",
      "iter  21:\t Loss = nan\n",
      "iter  22:\t Loss = nan\n",
      "iter  23:\t Loss = nan\n",
      "iter  24:\t Loss = nan\n",
      "iter  25:\t Loss = nan\n",
      "iter  26:\t Loss = nan\n",
      "iter  27:\t Loss = nan\n",
      "iter  28:\t Loss = nan\n",
      "iter  29:\t Loss = nan\n",
      "iter  30:\t Loss = nan\n",
      "iter  31:\t Loss = nan\n",
      "iter  32:\t Loss = nan\n",
      "iter  33:\t Loss = nan\n",
      "iter  34:\t Loss = nan\n",
      "iter  35:\t Loss = nan\n",
      "iter  36:\t Loss = nan\n",
      "iter  37:\t Loss = nan\n",
      "iter  38:\t Loss = nan\n",
      "iter  39:\t Loss = nan\n",
      "iter  40:\t Loss = nan\n",
      "iter  41:\t Loss = nan\n",
      "iter  42:\t Loss = nan\n",
      "iter  43:\t Loss = nan\n",
      "iter  44:\t Loss = nan\n",
      "iter  45:\t Loss = nan\n",
      "iter  46:\t Loss = nan\n",
      "iter  47:\t Loss = nan\n",
      "iter  48:\t Loss = nan\n",
      "iter  49:\t Loss = nan\n",
      "iter  50:\t Loss = nan\n",
      "iter  51:\t Loss = nan\n",
      "iter  52:\t Loss = nan\n",
      "-----------------------------------------------\n",
      "Training epoch :  8\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = nan\n",
      "iter   1:\t Loss = nan\n",
      "iter   2:\t Loss = nan\n",
      "iter   3:\t Loss = nan\n",
      "iter   4:\t Loss = nan\n",
      "iter   5:\t Loss = nan\n",
      "iter   6:\t Loss = nan\n",
      "iter   7:\t Loss = nan\n",
      "iter   8:\t Loss = nan\n",
      "iter   9:\t Loss = nan\n",
      "iter  10:\t Loss = nan\n",
      "iter  11:\t Loss = nan\n",
      "iter  12:\t Loss = nan\n",
      "iter  13:\t Loss = nan\n",
      "iter  14:\t Loss = nan\n",
      "iter  15:\t Loss = nan\n",
      "iter  16:\t Loss = nan\n",
      "iter  17:\t Loss = nan\n",
      "iter  18:\t Loss = nan\n",
      "iter  19:\t Loss = nan\n",
      "iter  20:\t Loss = nan\n",
      "iter  21:\t Loss = nan\n",
      "iter  22:\t Loss = nan\n",
      "iter  23:\t Loss = nan\n",
      "iter  24:\t Loss = nan\n",
      "iter  25:\t Loss = nan\n",
      "iter  26:\t Loss = nan\n",
      "iter  27:\t Loss = nan\n",
      "iter  28:\t Loss = nan\n",
      "iter  29:\t Loss = nan\n",
      "iter  30:\t Loss = nan\n",
      "iter  31:\t Loss = nan\n",
      "iter  32:\t Loss = nan\n",
      "iter  33:\t Loss = nan\n",
      "iter  34:\t Loss = nan\n",
      "iter  35:\t Loss = nan\n",
      "iter  36:\t Loss = nan\n",
      "iter  37:\t Loss = nan\n",
      "iter  38:\t Loss = nan\n",
      "iter  39:\t Loss = nan\n",
      "iter  40:\t Loss = nan\n",
      "iter  41:\t Loss = nan\n",
      "iter  42:\t Loss = nan\n",
      "iter  43:\t Loss = nan\n",
      "iter  44:\t Loss = nan\n",
      "iter  45:\t Loss = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cad3407779a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mfeed_dict_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mg_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mz_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mdisplay_freq\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initializing all variables\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "num_tr_iter=int(len(y_train)/batch_size)\n",
    "print('num_tr_iter = ', num_tr_iter)\n",
    "store_loss_train_name='global_loss_train.dat'\n",
    "sltn=open(store_loss_train_name,\"w+\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('-----------------------------------------------')\n",
    "    print('Training epoch : ', epoch)\n",
    "    print('-----------------------------------------------')\n",
    "    for iteration in range(num_tr_iter):\n",
    "        start=iteration*batch_size\n",
    "        end=(iteration+1)*batch_size\n",
    "        x_batch,dx_batch,z_batch,g_batch,y_batch=get_next_batch(x_train,y_train,start,end) \n",
    "        z_batch=np.expand_dims(z_batch,1)\n",
    "        dx_batch=np.expand_dims(dx_batch,1)\n",
    "        g_batch=np.expand_dims(g_batch,1)\n",
    "        y_batch=np.expand_dims(y_batch,1)\n",
    "\n",
    "        feed_dict_batch={x:x_batch,dx:dx_batch,g:g_batch,z:z_batch,y:y_batch}\n",
    "\n",
    "        sess.run(optimizer,feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration%display_freq==0:\n",
    "            loss_batch=sess.run(loss,feed_dict=feed_dict_batch)\n",
    "            print(\"iter {0:3d}:\\t Loss = {1:2f}\".format(iteration,loss_batch))\n",
    "            sltn.write(\"%6d    %15.12f\\n\" %(iteration+epoch*num_tr_iter, loss_batch))\n",
    "sltn.close()\n",
    "\n",
    "# plot training loss function\n",
    "#matplotlib.figure.Figure()\n",
    "xloss,yloss=np.loadtxt(store_loss_train_name,unpack=True)\n",
    "plt.plot(xloss, yloss)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# save and plot y_predict_train, y_train\n",
    "store_y_train_name='y_train.dat'\n",
    "sytn=open(store_y_train_name,\"w+\")\n",
    "np.random.seed(68721)\n",
    "random_list=np.random.randint(len(y_train),size=2000)\n",
    "x_batch=np.zeros([1000,x_train.shape[1]-1])\n",
    "xprime_batch=np.zeros([1000,1])\n",
    "y_batch=np.zeros([1000,1])\n",
    "for i in range(1000):\n",
    "    x_batch[i,0:x_train.shape[1]-1]=x_train[random_list[i],0:x_train.shape[1]-1].T\n",
    "    xprime_batch[i,0]=x_train[random_list[i],x_train.shape[1]-1]\n",
    "    y_batch[i,0]=y_train[random_list[i]]\n",
    "\n",
    "feed_dict_batch={x:x_batch,xprime:xprime_batch,y:y_batch}\n",
    "y_train_predict=sess.run(output,feed_dict=feed_dict_batch)\n",
    "print(y_train_predict.shape)\n",
    "for i in range(1000):\n",
    "    sytn.write(\"%15.12f    %15.12f\\n\" %(y_batch[i,0], y_train_predict[i,0]))\n",
    "sytn.close()\n",
    "\n",
    "#matplotlib.figure.Figure()\n",
    "y1, y2=np.loadtxt(store_y_train_name,unpack=True)\n",
    "plt.plot(y1, y2,'ro')\n",
    "plt.plot(y1,y1,'-b')\n",
    "plt.xlabel('Y Value')\n",
    "plt.ylabel('Y Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
