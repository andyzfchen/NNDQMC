{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thomas/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "(2, 4)\n",
      "[['X11' 'NN11' 'NNN11' 'X12' 'NN12' 'NNN12' 'X13' 'NN13' 'NNN13' 'X14'\n",
      "  'NN14' 'NNN14']\n",
      " ['X21' 'NN21' 'NNN21' 'X22' 'NN22' 'NNN22' 'X23' 'NN23' 'NNN23' 'X24'\n",
      "  'NN24' 'NNN24']]\n",
      "[['X11' 'NN11' 'NNN11']\n",
      " ['X12' 'NN12' 'NNN12']\n",
      " ['X13' 'NN13' 'NNN13']\n",
      " ['X14' 'NN14' 'NNN14']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "%matplotlib notebook\n",
    "tf.disable_v2_behavior()\n",
    "print(np.array([['X11','X12','X13','X14'],['X21','X22','X23','X24']]).shape)\n",
    "posdata = np.stack(([['X11','X12','X13','X14'],['X21','X22','X23','X24']],[['NN11','NN12','NN13','NN14'],['NN21','NN22','NN23','NN24']],[['NNN11','NNN12','NNN13','NNN14'],['NNN21','NNN22','NNN23','NNN24']]),axis=2)\n",
    "   \n",
    "posdata=np.reshape(posdata,(2,4*3))\n",
    "print(posdata)\n",
    "posdata=np.reshape(posdata[0],(4,3))\n",
    "print(posdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name,maxrow):\n",
    "    print(\"read data from \\\"\",file_name,\"\\\"\")\n",
    "    \n",
    "    xdata=np.loadtxt(file_name,max_rows=maxrow)\n",
    "    print(xdata.shape)\n",
    "    print(np.argwhere(np.isnan(xdata[:,1])))\n",
    "    xdata=np.delete(xdata,np.argwhere(np.isnan(xdata[:,1])),axis=0)\n",
    "    maxrow=xdata.shape[0]\n",
    "    print(xdata.shape)\n",
    "    L=(xdata.shape[1]-3)//9\n",
    "    print(L)\n",
    "    ydata=xdata[:,1]\n",
    "    dxdata=xdata[:,xdata.shape[1]-1]\n",
    "    \n",
    "    posdata=xdata[:,2+4*L:2+5*L]\n",
    "    NNdata=np.stack((xdata[:,2+L:2+2*L],xdata[:,2+3*L:2+4*L],xdata[:,2+5*L:2+6*L],xdata[:,2+7*L:2+8*L]),axis=2)\n",
    "    NNNdata=np.stack((xdata[:,2:2+L],xdata[:,2+2*L:2+3*L],xdata[:,2+6*L:2+7*L],xdata[:,2+8*L:2+9*L]),axis=2)\n",
    "    NNdata=np.mean(NNdata,axis=2)\n",
    "    NNNdata=np.mean(NNNdata,axis=2)\n",
    "    \n",
    "    averagex=np.mean(xdata[:,2:xdata.shape[1]-1])\n",
    "    print(averagex)\n",
    "    gdata_append=0.25*(posdata+np.outer(dxdata,np.ones(L))-averagex)**4-averagex**2/2.0*(posdata+np.outer(dxdata,np.ones(L))-averagex)**2-0.25*(posdata-averagex)**4+averagex**2/2.0*(posdata-averagex)**2\n",
    "    print(np.outer(dxdata,np.ones(L)).shape)\n",
    "    gdata_append=np.mean(gdata_append,axis=1)\n",
    "    zdata_append=np.sum((posdata+np.outer(dxdata,np.ones(L)))**2-posdata**2,axis=1)\n",
    "    print(posdata.shape)\n",
    "    posdata = np.stack((posdata,NNdata,NNNdata),axis=2)\n",
    "   \n",
    "    posdata=np.reshape(posdata,(maxrow,L*3))\n",
    "    \n",
    "    return posdata, dxdata, ydata, zdata_append, gdata_append\n",
    "\n",
    "\n",
    "def load_data2(file_name,maxrow):\n",
    "    print(\"read data from \\\"\",file_name,\"\\\"\")\n",
    "    \n",
    "    xdata=np.loadtxt(file_name,max_rows=maxrow)\n",
    "    L=(xdata.shape[1]-3)//9\n",
    "    print(L)\n",
    "    ydata=xdata[:,1]\n",
    "    dxdata=xdata[:,xdata.shape[1]-1]\n",
    "    \n",
    "    posdata=xdata[:,2+8*L:2+9*L]\n",
    "    NNdata=np.stack((xdata[:,2+L:2+2*L],xdata[:,2+3*L:2+4*L],xdata[:,2+4*L:2+5*L],xdata[:,2+6*L:2+7*L]),axis=2)\n",
    "    NNNdata=np.stack((xdata[:,2:2+L],xdata[:,2+2*L:2+3*L],xdata[:,2+5*L:2+6*L],xdata[:,2+7*L:2+8*L]),axis=2)\n",
    "    NNdata=np.mean(NNdata,axis=2)\n",
    "    NNNdata=np.mean(NNNdata,axis=2)\n",
    "    \n",
    "    averagex=np.mean(xdata[:,2:xdata.shape[1]-1])\n",
    "    print(averagex)\n",
    "    gdata_append=0.25*(posdata+np.outer(dxdata,np.ones(L))-averagex)**4-averagex**2/2.0*(posdata+np.outer(dxdata,np.ones(L))-averagex)**2-0.25*(posdata-averagex)**4+averagex**2/2.0*(posdata-averagex)**2\n",
    "    print(np.outer(dxdata,np.ones(L)).shape)\n",
    "    gdata_append=np.mean(gdata_append,axis=1)\n",
    "    zdata_append=np.sum((posdata+np.outer(dxdata,np.ones(L)))**2-posdata**2,axis=1)\n",
    "    print(posdata.shape)\n",
    "    posdata = np.stack((posdata,NNdata,NNNdata),axis=2)\n",
    "   \n",
    "    posdata=np.reshape(posdata,(maxrow,L*3))\n",
    "    \n",
    "    return posdata, dxdata, ydata, zdata_append, gdata_append\n",
    "\n",
    "\n",
    "\n",
    "def scaled_data(xdata):\n",
    "    xdata_mean=np.mean(xdata,axis=0)\n",
    "    xdata_std=np.std(xdata,axis=0)\n",
    "    xdata_scaled= (xdata-xdata_mean)/xdata_std\n",
    "    return xdata_scaled, xdata_mean, xdata_std\n",
    "\n",
    "\n",
    "def get_next_batch(x,y,start,end):\n",
    "    x_batch=x[start:end,0:x.shape[1]-3]\n",
    "    dx_batch=x[start:end,x.shape[1]-3]\n",
    "    z_batch=x[start:end,x.shape[1]-2]\n",
    "    g_batch=x[start:end,x.shape[1]-1]\n",
    "    y_batch=y[start:end]\n",
    "    return x_batch, dx_batch, z_batch, g_batch, y_batch\n",
    "\n",
    "\n",
    "def weight_variable(name, shape):\n",
    "    initer=tf.truncated_normal_initializer(stddev=0.02)\n",
    "    return tf.get_variable('W_'+name, dtype=tf.float32,shape=shape,initializer=initer)\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    initial=tf.constant(0.0,shape=shape,dtype=tf.float32)\n",
    "    return tf.get_variable('b_'+name,dtype=tf.float32,initializer=initial)\n",
    "\n",
    "def create_new_conv_layer(input_data, deltax, num_filters, filter_shape, name):\n",
    "    # setup the filter input shape for tf.nn.conv_2d\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], 1, num_filters]\n",
    "\n",
    "    # initialise weights and bias for the filter\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
    "                                      name=name+'_W')\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "\n",
    "    # setup the convolutional layer operation\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 3, 1, 1],padding='VALID')\n",
    "\n",
    "    # add the bias\n",
    "    out_layer += bias\n",
    "    \n",
    "    # add the theta term\n",
    "    in_dim=dx.get_shape()[1]\n",
    "    theta = tf.Variable(tf.truncated_normal([in_dim,num_filters]), name=name+'_theta')\n",
    "    out_layer +=tf.tensordot(tf.tensordot(tf.squeeze(dx),tf.ones([12],tf.float32),axes=0),theta,axes=0)\n",
    "\n",
    "    # apply a sigmoid non-linear activation\n",
    "    out_layer = tf.nn.softplus(out_layer)\n",
    "\n",
    "    \n",
    "    return out_layer, weights, bias, theta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fc_layer(x,num_units,name,activation):\n",
    "    in_dim=x.get_shape()[1]\n",
    "    W=weight_variable(name,shape=[in_dim,num_units])\n",
    "    b=bias_variable(name,[num_units])\n",
    "    layer=tf.matmul(x,W)\n",
    "    layer+=b\n",
    "    if activation=='softplus':\n",
    "        layer=tf.nn.tanh(layer)\n",
    "    elif activation=='sigmoid':\n",
    "        layer=tf.nn.tanh(layer)\n",
    "    return layer, W, b\n",
    "\n",
    "def fc_layer_final(x,g,z,num_units,name):\n",
    "    in_dim=x.get_shape()[1]\n",
    "    W1=weight_variable(name,shape=[in_dim,num_units])\n",
    "    W2g=weight_variable(name+'g',shape=[1,num_units])\n",
    "    W3z=weight_variable(name+'x',shape=[1,num_units])\n",
    "    b=bias_variable(name,[num_units])\n",
    "    \n",
    "    layer=tf.matmul(x,W1)+tf.matmul(g,W2g)+tf.matmul(z,W3z)\n",
    "\n",
    "    layer+=b\n",
    "    return layer, W1, W2g, W3z, b\n",
    "\n",
    "def save_data(file_name,name,x,matrix):\n",
    "    out_file=open(file_name,'a')\n",
    "    if matrix:\n",
    "        out_file.write(\"%30s: [  %4d  ,  %4d  ]\\n\"%(name,x.shape[0], x.shape[1]))\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                out_file.write(\"%10.6f    \" %(x[i,j]))\n",
    "            out_file.write(\"\\n\")\n",
    "    else:\n",
    "        out_file.write(\"%s: [  %4d  ]\\n\"%(name,x.shape[0]))\n",
    "        for i in range(x.shape[0]):\n",
    "            out_file.write(\"%10.6f    \"%(x[i]))\n",
    "        out_file.write(\"\\n\")\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "file_name=\"./traindata/build/globaltrain.dat\"\n",
    "#file_name=\"./Desktop/block_train0003.dat\"\n",
    "\n",
    "maxrow=100000\n",
    "\n",
    "# set hyper parameters\n",
    "epochs=400\n",
    "batch_size=200\n",
    "display_freq=100\n",
    "learning_rate=0.0005\n",
    "L2_regularization=0.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data from \" ./traindata/build/globaltrain.dat \"\n",
      "(100000, 372)\n",
      "[[ 1858]\n",
      " [ 2102]\n",
      " [ 3883]\n",
      " [ 4593]\n",
      " [ 6848]\n",
      " [ 7661]\n",
      " [10873]\n",
      " [11204]\n",
      " [11641]\n",
      " [11676]\n",
      " [13452]\n",
      " [20589]\n",
      " [32514]\n",
      " [33842]\n",
      " [34666]\n",
      " [34670]\n",
      " [34671]\n",
      " [34675]\n",
      " [34676]\n",
      " [45141]\n",
      " [46334]\n",
      " [50403]\n",
      " [52522]\n",
      " [52611]\n",
      " [53808]\n",
      " [54427]\n",
      " [58048]\n",
      " [60096]\n",
      " [62789]\n",
      " [63246]\n",
      " [64427]\n",
      " [69353]\n",
      " [70425]\n",
      " [71789]\n",
      " [72850]\n",
      " [73937]\n",
      " [78008]\n",
      " [79561]\n",
      " [86044]\n",
      " [87830]\n",
      " [88963]\n",
      " [95384]\n",
      " [96311]\n",
      " [98784]]\n",
      "(99956, 372)\n",
      "41\n",
      "-3.9996732015947853\n",
      "(99956, 41)\n",
      "(99956, 41)\n",
      "Size of:\n",
      "x_train:\t(66970, 126)\n",
      "y_train:\t(66970,)\n",
      "x_test:\t(32986, 126)\n",
      "y_test:\t(32986,)\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "xdata, dxdata, ydata, zdata_append, gdata_append=load_data(file_name,maxrow)\n",
    "#xdata, dxdata, ydata, zdata_append, gdata_append=load_data2(file_name,maxrow)\n",
    "\n",
    "# scaled data\n",
    "xdata_scaled,        xdata_mean,        xdata_std        = scaled_data(xdata)\n",
    "zdata_append_scaled, zdata_append_mean, zdata_append_std = scaled_data(zdata_append)\n",
    "gdata_append_scaled, gdata_append_mean, gdata_append_std = scaled_data(gdata_append)\n",
    "dxdata_scaled,       dxdata_mean,       dxdata_std       = scaled_data(dxdata)\n",
    "xdata_combined=np.concatenate((xdata_scaled,np.array([dxdata_scaled]).T,np.array([zdata_append_scaled]).T,np.array([gdata_append_scaled]).T),axis=1)\n",
    "\n",
    "# split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(xdata_combined, ydata, test_size=0.33, random_state=42)\n",
    "L = (x_train.shape[1]-3)//3\n",
    "print(\"Size of:\")\n",
    "print('x_train:\\t{}'.format(x_train.shape))\n",
    "print('y_train:\\t{}'.format(y_train.shape))\n",
    "print('x_test:\\t{}'.format(x_test.shape))\n",
    "print('y_test:\\t{}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thomas/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py:4022: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
      "WARNING:tensorflow:From <ipython-input-5-26d9e26c41a8>:11: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
      "Instructions for updating:\n",
      "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create placeholders\n",
    "x = tf.placeholder(tf.float32,shape=[None, x_train.shape[1]-3],name='X')\n",
    "x_shaped = tf.reshape(x,[-1,L,3,1])\n",
    "dx = tf.placeholder(tf.float32,shape=[None, 1] ,name='dX')\n",
    "g = tf.placeholder(tf.float32,shape=[None, 1] ,name='g')\n",
    "z = tf.placeholder(tf.float32,shape=[None, 1] ,name='z')\n",
    "y = tf.placeholder(tf.float32,shape=[None,1] ,name='Y')\n",
    "\n",
    "#define the network\n",
    "convout, Wconv, bconv, thetaconv = create_new_conv_layer(x_shaped, dx, 10, [8,3], name=\"convlayer\")\n",
    "print_convout = tf.Print(convout,[tf.shape(convout)],\"The shape of conv out; \",summarize=4)\n",
    "flattened = tf.reshape(print_convout, [-1, 10*((L-8)//3+1)])\n",
    "fc1, W1, b1 = fc_layer(flattened, 20 , 'FC1' , 'sigmoid')\n",
    "fcib, Wib, bib = fc_layer(fc1, 10 , 'FCib' , 'sigmoid')\n",
    "\n",
    "output, W2, W2g, W2z, b2 = fc_layer_final(fcib,g,z,1,'FC2')\n",
    "\n",
    "\n",
    "loss=tf.reduce_mean(tf.squared_difference(output, y))\n",
    "regularizer=tf.reduce_sum([tf.nn.l2_loss(W1), tf.nn.l2_loss(W2), tf.nn.l2_loss(W2g), tf.nn.l2_loss(W2z), tf.nn.l2_loss(Wconv), tf.nn.l2_loss(thetaconv)])\n",
    "total_loss=tf.reduce_sum(loss+L2_regularization*regularizer)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate,name=\"Adam_op\").minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tr_iter =  334\n",
      "-----------------------------------------------\n",
      "Training epoch :  0\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.724691\n",
      "iter 100:\t Loss = 0.469144\n",
      "iter 200:\t Loss = 0.293351\n",
      "iter 300:\t Loss = 0.129528\n",
      "-----------------------------------------------\n",
      "Training epoch :  1\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.138901\n",
      "iter 100:\t Loss = 0.114841\n",
      "iter 200:\t Loss = 0.112655\n",
      "iter 300:\t Loss = 0.088854\n",
      "-----------------------------------------------\n",
      "Training epoch :  2\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.095817\n",
      "iter 100:\t Loss = 0.097188\n",
      "iter 200:\t Loss = 0.089308\n",
      "iter 300:\t Loss = 0.092458\n",
      "-----------------------------------------------\n",
      "Training epoch :  3\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.103105\n",
      "iter 100:\t Loss = 0.110046\n",
      "iter 200:\t Loss = 0.077708\n",
      "iter 300:\t Loss = 0.052022\n",
      "-----------------------------------------------\n",
      "Training epoch :  4\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.073056\n",
      "iter 100:\t Loss = 0.038181\n",
      "iter 200:\t Loss = 0.049012\n",
      "iter 300:\t Loss = 0.059219\n",
      "-----------------------------------------------\n",
      "Training epoch :  5\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.053577\n",
      "iter 100:\t Loss = 0.043190\n",
      "iter 200:\t Loss = 0.025965\n",
      "iter 300:\t Loss = 0.024186\n",
      "-----------------------------------------------\n",
      "Training epoch :  6\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.022737\n",
      "iter 100:\t Loss = 0.019067\n",
      "iter 200:\t Loss = 0.020027\n",
      "iter 300:\t Loss = 0.007748\n",
      "-----------------------------------------------\n",
      "Training epoch :  7\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.009931\n",
      "iter 100:\t Loss = 0.013194\n",
      "iter 200:\t Loss = 0.014840\n",
      "iter 300:\t Loss = 0.010273\n",
      "-----------------------------------------------\n",
      "Training epoch :  8\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.013805\n",
      "iter 100:\t Loss = 0.019001\n",
      "iter 200:\t Loss = 0.011500\n",
      "iter 300:\t Loss = 0.007345\n",
      "-----------------------------------------------\n",
      "Training epoch :  9\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.006429\n",
      "iter 100:\t Loss = 0.011313\n",
      "iter 200:\t Loss = 0.006080\n",
      "iter 300:\t Loss = 0.058844\n",
      "-----------------------------------------------\n",
      "Training epoch :  10\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.008728\n",
      "iter 100:\t Loss = 0.006484\n",
      "iter 200:\t Loss = 0.008447\n",
      "iter 300:\t Loss = 0.007339\n",
      "-----------------------------------------------\n",
      "Training epoch :  11\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.006901\n",
      "iter 100:\t Loss = 0.009107\n",
      "iter 200:\t Loss = 0.006140\n",
      "iter 300:\t Loss = 0.004902\n",
      "-----------------------------------------------\n",
      "Training epoch :  12\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003491\n",
      "iter 100:\t Loss = 0.006406\n",
      "iter 200:\t Loss = 0.012093\n",
      "iter 300:\t Loss = 0.003249\n",
      "-----------------------------------------------\n",
      "Training epoch :  13\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.006040\n",
      "iter 100:\t Loss = 0.003350\n",
      "iter 200:\t Loss = 0.003603\n",
      "iter 300:\t Loss = 0.003972\n",
      "-----------------------------------------------\n",
      "Training epoch :  14\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.004398\n",
      "iter 100:\t Loss = 0.004531\n",
      "iter 200:\t Loss = 0.003814\n",
      "iter 300:\t Loss = 0.003705\n",
      "-----------------------------------------------\n",
      "Training epoch :  15\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.016228\n",
      "iter 100:\t Loss = 0.004840\n",
      "iter 200:\t Loss = 0.003054\n",
      "iter 300:\t Loss = 0.004599\n",
      "-----------------------------------------------\n",
      "Training epoch :  16\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003506\n",
      "iter 100:\t Loss = 0.003547\n",
      "iter 200:\t Loss = 0.004577\n",
      "iter 300:\t Loss = 0.005207\n",
      "-----------------------------------------------\n",
      "Training epoch :  17\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.002393\n",
      "iter 100:\t Loss = 0.019827\n",
      "iter 200:\t Loss = 0.003378\n",
      "iter 300:\t Loss = 0.004935\n",
      "-----------------------------------------------\n",
      "Training epoch :  18\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003183\n",
      "iter 100:\t Loss = 0.003315\n",
      "iter 200:\t Loss = 0.004535\n",
      "iter 300:\t Loss = 0.056095\n",
      "-----------------------------------------------\n",
      "Training epoch :  19\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.005225\n",
      "iter 100:\t Loss = 0.003089\n",
      "iter 200:\t Loss = 0.003084\n",
      "iter 300:\t Loss = 0.004139\n",
      "-----------------------------------------------\n",
      "Training epoch :  20\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003630\n",
      "iter 100:\t Loss = 0.003094\n",
      "iter 200:\t Loss = 0.004486\n",
      "iter 300:\t Loss = 0.003505\n",
      "-----------------------------------------------\n",
      "Training epoch :  21\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003047\n",
      "iter 100:\t Loss = 0.002401\n",
      "iter 200:\t Loss = 0.005065\n",
      "iter 300:\t Loss = 0.003896\n",
      "-----------------------------------------------\n",
      "Training epoch :  22\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003201\n",
      "iter 100:\t Loss = 0.003616\n",
      "iter 200:\t Loss = 0.002503\n",
      "iter 300:\t Loss = 0.003831\n",
      "-----------------------------------------------\n",
      "Training epoch :  23\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.004628\n",
      "iter 100:\t Loss = 0.002702\n",
      "iter 200:\t Loss = 0.003163\n",
      "iter 300:\t Loss = 0.003100\n",
      "-----------------------------------------------\n",
      "Training epoch :  24\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003269\n",
      "iter 100:\t Loss = 0.002303\n",
      "iter 200:\t Loss = 0.003253\n",
      "iter 300:\t Loss = 0.002938\n",
      "-----------------------------------------------\n",
      "Training epoch :  25\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.009429\n",
      "iter 100:\t Loss = 0.004988\n",
      "iter 200:\t Loss = 0.001629\n",
      "iter 300:\t Loss = 0.002919\n",
      "-----------------------------------------------\n",
      "Training epoch :  26\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.001944\n",
      "iter 100:\t Loss = 0.003346\n",
      "iter 200:\t Loss = 0.005082\n",
      "iter 300:\t Loss = 0.003614\n",
      "-----------------------------------------------\n",
      "Training epoch :  27\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003813\n",
      "iter 100:\t Loss = 0.004776\n",
      "iter 200:\t Loss = 0.003918\n",
      "iter 300:\t Loss = 0.007011\n",
      "-----------------------------------------------\n",
      "Training epoch :  28\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003296\n",
      "iter 100:\t Loss = 0.002531\n",
      "iter 200:\t Loss = 0.005797\n",
      "iter 300:\t Loss = 0.008735\n",
      "-----------------------------------------------\n",
      "Training epoch :  29\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003164\n",
      "iter 100:\t Loss = 0.013737\n",
      "iter 200:\t Loss = 0.003800\n",
      "iter 300:\t Loss = 0.002897\n",
      "-----------------------------------------------\n",
      "Training epoch :  30\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003507\n",
      "iter 100:\t Loss = 0.002486\n",
      "iter 200:\t Loss = 0.002583\n",
      "iter 300:\t Loss = 0.003130\n",
      "-----------------------------------------------\n",
      "Training epoch :  31\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003206\n",
      "iter 100:\t Loss = 0.002929\n",
      "iter 200:\t Loss = 0.003728\n",
      "iter 300:\t Loss = 0.006214\n",
      "-----------------------------------------------\n",
      "Training epoch :  32\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.006789\n",
      "iter 100:\t Loss = 0.002575\n",
      "iter 200:\t Loss = 0.003525\n",
      "iter 300:\t Loss = 0.003289\n",
      "-----------------------------------------------\n",
      "Training epoch :  33\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.021000\n",
      "iter 100:\t Loss = 0.002755\n",
      "iter 200:\t Loss = 0.004266\n",
      "iter 300:\t Loss = 0.001809\n",
      "-----------------------------------------------\n",
      "Training epoch :  34\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003501\n",
      "iter 100:\t Loss = 0.007928\n",
      "iter 200:\t Loss = 0.002192\n",
      "iter 300:\t Loss = 0.004082\n",
      "-----------------------------------------------\n",
      "Training epoch :  35\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.004246\n",
      "iter 100:\t Loss = 0.002317\n",
      "iter 200:\t Loss = 0.004953\n",
      "iter 300:\t Loss = 0.015966\n",
      "-----------------------------------------------\n",
      "Training epoch :  36\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.008176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100:\t Loss = 0.003173\n",
      "iter 200:\t Loss = 0.003923\n",
      "iter 300:\t Loss = 0.002618\n",
      "-----------------------------------------------\n",
      "Training epoch :  37\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003218\n",
      "iter 100:\t Loss = 0.004438\n",
      "iter 200:\t Loss = 0.003198\n",
      "iter 300:\t Loss = 0.004230\n",
      "-----------------------------------------------\n",
      "Training epoch :  38\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.004293\n",
      "iter 100:\t Loss = 0.003759\n",
      "iter 200:\t Loss = 0.022747\n",
      "iter 300:\t Loss = 0.003919\n",
      "-----------------------------------------------\n",
      "Training epoch :  39\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.006971\n",
      "iter 100:\t Loss = 0.002495\n",
      "iter 200:\t Loss = 0.003308\n",
      "iter 300:\t Loss = 0.007481\n",
      "-----------------------------------------------\n",
      "Training epoch :  40\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.002064\n",
      "iter 100:\t Loss = 0.002833\n",
      "iter 200:\t Loss = 0.003688\n",
      "iter 300:\t Loss = 0.002781\n",
      "-----------------------------------------------\n",
      "Training epoch :  41\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.002791\n",
      "iter 100:\t Loss = 0.002302\n",
      "iter 200:\t Loss = 0.003178\n",
      "iter 300:\t Loss = 0.003258\n",
      "-----------------------------------------------\n",
      "Training epoch :  42\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.010377\n",
      "iter 100:\t Loss = 0.001943\n",
      "iter 200:\t Loss = 0.003381\n",
      "iter 300:\t Loss = 0.049028\n",
      "-----------------------------------------------\n",
      "Training epoch :  43\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.002834\n",
      "iter 100:\t Loss = 0.002562\n",
      "iter 200:\t Loss = 0.002920\n",
      "iter 300:\t Loss = 0.003575\n",
      "-----------------------------------------------\n",
      "Training epoch :  44\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003949\n",
      "iter 100:\t Loss = 0.003040\n",
      "iter 200:\t Loss = 0.003308\n",
      "iter 300:\t Loss = 0.003085\n",
      "-----------------------------------------------\n",
      "Training epoch :  45\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003168\n",
      "iter 100:\t Loss = 0.004610\n",
      "iter 200:\t Loss = 0.003329\n",
      "iter 300:\t Loss = 0.002526\n",
      "-----------------------------------------------\n",
      "Training epoch :  46\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.002254\n",
      "iter 100:\t Loss = 0.010525\n",
      "iter 200:\t Loss = 0.002724\n",
      "iter 300:\t Loss = 0.002915\n",
      "-----------------------------------------------\n",
      "Training epoch :  47\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.004080\n",
      "iter 100:\t Loss = 0.003111\n",
      "iter 200:\t Loss = 0.005028\n",
      "iter 300:\t Loss = 0.003209\n",
      "-----------------------------------------------\n",
      "Training epoch :  48\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.004300\n",
      "iter 100:\t Loss = 0.003379\n",
      "iter 200:\t Loss = 0.004314\n",
      "iter 300:\t Loss = 0.002961\n",
      "-----------------------------------------------\n",
      "Training epoch :  49\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003601\n",
      "iter 100:\t Loss = 0.003014\n",
      "iter 200:\t Loss = 0.003567\n",
      "iter 300:\t Loss = 0.002866\n",
      "-----------------------------------------------\n",
      "Training epoch :  50\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.002967\n",
      "iter 100:\t Loss = 0.003607\n",
      "iter 200:\t Loss = 0.002840\n",
      "iter 300:\t Loss = 0.002841\n",
      "-----------------------------------------------\n",
      "Training epoch :  51\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.002379\n",
      "iter 100:\t Loss = 0.003216\n",
      "iter 200:\t Loss = 0.006957\n",
      "iter 300:\t Loss = 0.003172\n",
      "-----------------------------------------------\n",
      "Training epoch :  52\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.002405\n",
      "iter 100:\t Loss = 0.003229\n",
      "iter 200:\t Loss = 0.008828\n",
      "iter 300:\t Loss = 0.002225\n",
      "-----------------------------------------------\n",
      "Training epoch :  53\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003626\n",
      "iter 100:\t Loss = 0.002371\n",
      "iter 200:\t Loss = 0.003670\n",
      "iter 300:\t Loss = 0.003095\n",
      "-----------------------------------------------\n",
      "Training epoch :  54\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.001960\n",
      "iter 100:\t Loss = 0.003711\n",
      "iter 200:\t Loss = 0.003158\n",
      "iter 300:\t Loss = 0.001955\n",
      "-----------------------------------------------\n",
      "Training epoch :  55\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.002778\n",
      "iter 100:\t Loss = 0.002950\n",
      "iter 200:\t Loss = 0.004713\n",
      "iter 300:\t Loss = 0.003551\n",
      "-----------------------------------------------\n",
      "Training epoch :  56\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.003413\n",
      "iter 100:\t Loss = 0.006831\n",
      "iter 200:\t Loss = 0.002268\n",
      "iter 300:\t Loss = 0.002898\n",
      "-----------------------------------------------\n",
      "Training epoch :  57\n",
      "-----------------------------------------------\n",
      "iter   0:\t Loss = 0.002468\n",
      "iter 100:\t Loss = 0.002911\n",
      "iter 200:\t Loss = 0.004786\n"
     ]
    }
   ],
   "source": [
    "# initializing all variables\n",
    "init=tf.global_variables_initializer()\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "num_tr_iter=int(len(y_train)/batch_size)\n",
    "print('num_tr_iter = ', num_tr_iter)\n",
    "store_loss_train_name='global_loss_train.dat'\n",
    "sltn=open(store_loss_train_name,\"w+\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('-----------------------------------------------')\n",
    "    print('Training epoch : ', epoch)\n",
    "    print('-----------------------------------------------')\n",
    "    perm = np.arange(y_train.shape[0])\n",
    "    np.random.shuffle(perm)\n",
    "    y_train = y_train[perm]\n",
    "    x_train = x_train[perm]\n",
    "    for iteration in range(num_tr_iter):\n",
    "        \n",
    "        start=iteration*batch_size\n",
    "        end=(iteration+1)*batch_size\n",
    "        x_batch,dx_batch,z_batch,g_batch,y_batch=get_next_batch(x_train,y_train,start,end)\n",
    "        z_batch=np.expand_dims(z_batch,1)\n",
    "        dx_batch=np.expand_dims(dx_batch,1)\n",
    "        g_batch=np.expand_dims(g_batch,1)\n",
    "        y_batch=np.expand_dims(y_batch,1)\n",
    "\n",
    "        feed_dict_batch={x:x_batch,dx:dx_batch,g:g_batch,z:z_batch,y:y_batch}\n",
    "\n",
    "        sess.run(optimizer,feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration%display_freq==0:\n",
    "            loss_batch=sess.run(loss,feed_dict=feed_dict_batch)\n",
    "            print(\"iter {0:3d}:\\t Loss = {1:2f}\".format(iteration,loss_batch))\n",
    "            sltn.write(\"%6d    %15.12f\\n\" %(iteration+epoch*num_tr_iter, loss_batch))\n",
    "sltn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss function\n",
    "#matplotlib.figure.Figure()\n",
    "xloss,yloss=np.loadtxt(store_loss_train_name,unpack=True)\n",
    "plt.plot(xloss, yloss)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Step',fontsize=18)\n",
    "plt.ylabel('Loss',fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "# save and plot y_predict_train, y_train\n",
    "store_y_train_name='y_train.dat'\n",
    "sytn=open(store_y_train_name,\"w+\")\n",
    "np.random.seed(68721)\n",
    "random_list=np.random.randint(len(y_train),size=2000)\n",
    "x_batch=np.zeros([1000,x_train.shape[1]-3])\n",
    "g_batch=np.zeros([1000,1])\n",
    "dx_batch=np.zeros([1000,1])\n",
    "z_batch=np.zeros([1000,1])\n",
    "y_batch=np.zeros([1000,1])\n",
    "for i in range(1000):\n",
    "    x_batch[i,0:x_train.shape[1]-3]=x_train[random_list[i],0:x_train.shape[1]-3].T\n",
    "    g_batch[i,0]=x_train[random_list[i],x_train.shape[1]-1]\n",
    "    z_batch[i,0]=x_train[random_list[i],x_train.shape[1]-2]\n",
    "    dx_batch[i,0]=x_train[random_list[i],x_train.shape[1]-3]\n",
    "    y_batch[i,0]=y_train[random_list[i]]\n",
    "\n",
    "feed_dict_batch={x:x_batch,dx:dx_batch,g:g_batch,z:z_batch,y:y_batch}\n",
    "y_train_predict=sess.run(output,feed_dict=feed_dict_batch)\n",
    "print(y_train_predict.shape)\n",
    "for i in range(1000):\n",
    "    sytn.write(\"%15.12f    %15.12f\\n\" %(y_batch[i,0], y_train_predict[i,0]))\n",
    "sytn.close()\n",
    "\n",
    "#matplotlib.figure.Figure()\n",
    "y1, y2=np.loadtxt(store_y_train_name,unpack=True)\n",
    "plt.plot(y1, y2,'ro')\n",
    "plt.plot(y1,y1,'-b')\n",
    "plt.xlabel('$\\Delta$E Value',fontsize=18)\n",
    "plt.ylabel('$\\Delta$E Prediction',fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch,dx_batch,z_batch,g_batch,y_batch=get_next_batch(x_test,y_test,0,len(y_test))\n",
    "dx_batch=np.expand_dims(dx_batch,1)\n",
    "y_batch=np.expand_dims(y_batch,1)\n",
    "g_batch=np.expand_dims(g_batch,1)\n",
    "z_batch=np.expand_dims(z_batch,1)\n",
    "\n",
    "feed_dict_batch={x:x_batch,dx:dx_batch,g:g_batch,z:z_batch,y:y_batch}\n",
    "loss_test=sess.run(loss,feed_dict=feed_dict_batch)\n",
    "print(\"Test Loss = \",loss_test,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"variable list:\\n\")\n",
    "print(tf.trainable_variables())\n",
    "W_conv_ = [v for v in tf.trainable_variables() if v.name == \"convlayer_W:0\"][0]\n",
    "b_conv_ = [v for v in tf.trainable_variables() if v.name == \"convlayer_b:0\"][0]\n",
    "t_conv_ = [v for v in tf.trainable_variables() if v.name == \"convlayer_theta:0\"][0]\n",
    "\n",
    "W_FC1_ = [v for v in tf.trainable_variables() if v.name == \"W_FC1:0\"][0]\n",
    "b_FC1_ = [v for v in tf.trainable_variables() if v.name == \"b_FC1:0\"][0]\n",
    "W_FC2_ = [v for v in tf.trainable_variables() if v.name == \"W_FC2:0\"][0]\n",
    "W_FC2g_ = [v for v in tf.trainable_variables() if v.name == \"W_FC2g:0\"][0]\n",
    "W_FC2z_ = [v for v in tf.trainable_variables() if v.name == \"W_FC2x:0\"][0]\n",
    "b_FC2_ = [v for v in tf.trainable_variables() if v.name == \"b_FC2:0\"][0]\n",
    "\n",
    "W_conv=W_conv_.eval(session=sess)\n",
    "b_conv=b_conv_.eval(session=sess)\n",
    "t_conv=t_conv_.eval(session=sess)\n",
    "\n",
    "W_FC1=W_FC1_.eval(session=sess)\n",
    "b_FC1=b_FC1_.eval(session=sess)\n",
    "W_FC2=W_FC2_.eval(session=sess)\n",
    "b_FC2=b_FC2_.eval(session=sess)\n",
    "W_FC2g=W_FC2g_.eval(session=sess)\n",
    "W_FC2z=W_FC2z_.eval(session=sess)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(gdata_append_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_FC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name=\"L41global_hyper_parameter.dat\"\n",
    "if os.path.exists(output_file_name):\n",
    "    os.remove(output_file_name)\n",
    "f = open(output_file_name, \"a\")\n",
    "for i in range(10):\n",
    "    for j in range(8):\n",
    "        for k in range(3):\n",
    "            f.write(str(W_conv[j,k,0,i])+' ')\n",
    "f.write('\\n')\n",
    "for i in range(120):\n",
    "    for j in range(20):\n",
    "        f.write(str(W_FC1[i,j])+' ')\n",
    "    f.write('\\n')\n",
    "for j in range(20):\n",
    "    f.write(str(W_FC2[j,0])+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(W_FC2g[0,0])+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(W_FC2z[0,0])+' ')\n",
    "f.write('\\n')\n",
    "for j in range(10):\n",
    "    f.write(str(t_conv[0,j])+' ')\n",
    "f.write('\\n')\n",
    "for j in range(10):\n",
    "    f.write(str(b_conv[j])+' ')\n",
    "f.write('\\n')\n",
    "for j in range(20):\n",
    "    f.write(str(b_FC1[j])+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(b_FC2[0])+' ')\n",
    "f.write('\\n')\n",
    "for i in range(123):\n",
    "    f.write(str(xdata_mean[i])+' ')\n",
    "f.write('\\n')\n",
    "for i in range(123):\n",
    "    f.write(str(xdata_std[i])+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(dxdata_mean)+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(dxdata_std)+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(zdata_append_mean)+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(zdata_append_std)+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(gdata_append_mean)+' ')\n",
    "f.write('\\n')\n",
    "f.write(str(gdata_append_std)+' ')\n",
    "f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
